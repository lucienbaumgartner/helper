---
title: "Google Search Crawler, HTTP Status Code Checks & Text Extraction [BETA]"
author: "Lucien Baumgartner"
date: "10/10/2018"
output: 
  html_document:
    theme: readable 
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

```{css echo=F}
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

pre.html{
  background-color:white;
}

pre.r{
  background-color:black;
  color:white;
}
```

```{r}
library(dplyr)
library(XML)
library(RCurl)
library(pbapply)

rm(list=ls())

source('~/r-helpers/google-search/google-search-fx.R')
source('~/r-helpers/selenium-hacks/check-http-status-codes-fx.R')
source('~/r-helpers/selenium-hacks/extract-txt-fx.R')

### custom functions
get_search_url
get_google_hits
check_status_code

# enter search term
search.term <- "NoBillag"

## specify the URL for searches:
# quotes:: quoted search term (T/F)
# n.pages:: number pages that should be returned; if n.pages > 1 -> additional urls are gerated for each page
search.url <- get_search_url(search.term=search.term, quotes=F, n.pages=3)

search.url

## get hits back
# raw:: if you want the raw url (T/F)
# drop.recursive:: if you want to drop results from picture result suggestions etc. (T/F)
hits <- pblapply(search.url, function(x) get_google_hits(x, raw=F, drop.recursives = T))

hits

# check if they can be accessed or are dead links (is able to handle lists and vectors!)
hits.b <- check_status_code(hits) %>% print(n=100)

## get texts
# add.queries:: add additional queries; default: //p & //title
# preproc.expr:: additional regex expressions for preprocessing
txt <- extract_txt(hits.b$url[hits.b$boolean==T], 
                   add.queries = c(h2 = '//h2', h3 = '//h3'), 
                   preproc.expr = '(^(\\s+)?$)|(\\\n(\\s+)?)')
cat(txt[1])
```


