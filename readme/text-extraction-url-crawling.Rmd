---
title: "Google Search crawling, HTTP status checks, URL snowballing, text scraping, and document burning"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F)
```

```{css echo=F}
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

pre.html{
  background-color:white;
}

pre.r{
  background-color:black;
  color:white;
}
```

```{r echo=F}
source('~/r-helpers/google-search/google-search-fx.R')
source('~/r-helpers/selenium-hacks/check-http-status-codes-fx.R')
source('~/r-helpers/selenium-hacks/extract-txt-fx.R')
source('~/r-helpers/text-analysis/text-cleaning-fx.R')
source('~/r-helpers/url-snowballing/url-snowballing-fx.R')
```

# Overview

The following procedure has been developed to provide a stable URL collection method than can be coupled with text extraction and text burning features. The workflow has the following structure:

```
[[GOOGLE SEARCH CRAWLING]]
_ create Google Search URLs
  |_ extract top hits from n pages
    x .. [[HTTP STATUS CHECKS]]
    |_ check whether the webpages are still available
      |_ y: keep
        x .. [[URL SNOWBALLING]]
        |_ for ervery google hit; SET: 
          |_ URL restriction parameter 
            [keywords that have to be in the URL for it to be added to the collection]
          |_ time limit for QUERY
            [from root URL]
        |_ for ervery google hit; DO.CALL: snowballing algorithm
            |_ access root URL; extract all hrefs
            |_ add hrefs to LOG-file{ONLY IF: new}
              |_ access new hrefs from LOG; extract all hrefs
              |_ add hrefs to LOG-file{ONLY IF: new}
                |_ .. UNTIL no new unique hrefs
                    .. BREAK{IF: time reached before collection completed}
                      [prevents endless crawling]
                      x .. [[HTTP STATUS CHECKS]]
                      |_ check whether the webpages are still available
                        |_ y: keep
                          ///// OPTIONAL: .. [[TEXT MINING]]
                          x
                          |_ exctract texts; SET:
                            |_ HTML elements that should be extracted
                            |_ preprocessing regex
                            |_ merged (y/n) [single/multielement]
                              x
                              |_ burning algorithm; SET PARAM
                                ///// for EXPL see below
                        |_ n: drop 
      |_ n: drop 
```

# Functions

These are the functions I've written for each task

## Google Search crawling

### Get search URLs
```{r}
get_search_url
```

### Get top n pages of hits
```{r}
get_google_hits
```


## HTTP status checks
```{r}
check_status_code
```


## URL snowballing

### Snowballing algorithm
```{r}
snowballer
```

### Cycler

```{r}
cycler
```

## Text mining


### Text extraction
```{r}
extract_txt
```

### Text burning
The text burning fuction has several parameters:

```
# raw:: if T: only returns annotated texts (tibble); if F: actually computes transformations
# hard.filter:: if grepl returns true on any of these elements, the respective line (== HTML-element) will be removed (default=NULL)
# buzzwords:: additional buzzwords (default can be accessed by running `default.buzzwords`)
#         |_ buzzwords are used to indicate possible sclicing points and to compute filter conditions
# min.words:: minimum number of words per single character string (used for filtering, default: 3)
# min.avg.characters:: minimum number of avg.characters per word for each single character string (used for filtering, default: 3)
# max.buzzwords:: max. number of buzzwords allowed per character string (default: 2)
# sclicing:: whether slicing is allowed or not [slicing==deleting last part of text according to user-specified conditions] (T/F)
# slicing.keywords:: character string with additional slicing keywords (default can be accessed by running `default.slicing.keywords`)
#         |_ [!!!] each slicing keywords have to be identical to a buzzword
#         |_ [!!!] HENCE: sclicing.keyword is a subset of buzzwords
#         |_ [!!!] it is recommended to use them very cautiously, since they lead to deletions of whole text parts
# scnd.step.slicing:: last share of the document that will be used to compute the sum buzzwords (default: 3; meaning: last third of the document)
# scnd.step.threshold:: max number of buzzwords allowed in scnd.step.slicing before proceeding to slice the data (default: 4)
```

```{r}
clean_text
```

The logic behind the burning:

```
- collect parameters
  x
  |_ split strings into word-based lists and create burning template; CONTAINS:
    |_ raw text
    |_ number of words per string
    |_ average number of characters per word / string
    |_ buzzword matches
    |_ number of buzzword matches
    |_ string id
      x
      |_IF isTRUE(raw):: return burning template
      |_ELSE
        x .. [[CONDITIONAL SLICING]]
        |_IF isTRUE(slicing) & there are slicing keywords in the documents:: 
          |_IF a buzzword exists more than once:: select index of last occurence as slicing point
          |_ELSE select index of the occurence as slicing point
        x .. [[SOFT FILTERING]]  
        |_ remove all strings for which isTRUE(n.words<min.words|nchar.words.mean<min.avg.characters|n.buzzwords>=max.buzzwords)
        x .. [[HARD FILTERING]]
        |_IF !is.null(hard.filer):: remove all strings for which isTRUE(grepl(paste0(hard.filter, collapse = '|'), ..))
        x .. [[SECOND STEP SLICING]]
        |_IF isTRUE(slicing) & the sum of all buzzwords in the user-defined tailing portion of the text (==scnd.step.slicing) exceeds the userdefined threshold (==scnd.step.threshold):: remove tail
```

Dependencies:

```{r}
buzz_matches
```

# Display Case 
```{r eval=F}
library(XML)
library(RCurl)
library(pbapply)
library(dplyr)
library(stringi)
library(pbmcapply)
library(textcat)


rm(list=ls())

source('~/r-helpers/google-search/google-search-fx.R')
source('~/r-helpers/selenium-hacks/check-http-status-codes-fx.R')
source('~/r-helpers/selenium-hacks/extract-txt-fx.R')
source('~/r-helpers/text-analysis/text-cleaning-fx.R')
source('~/r-helpers/url-snowballing/url-snowballing-fx.R')

setwd('~/share/socsc1/output/')
# enter search term
search.term <- "No Billag"

## specify the URL for searches:
# quotes:: quoted search term (T/F)
# n.pages:: number pages that should be returned; if n.pages > 1 -> additional urls are gerated for each page
search.url <- get_search_url(search.term=search.term, language = 'de', quotes=F, n.pages=400)

## get hits back
# raw:: if you want the raw url (T/F)
# drop.recursive:: if you want to drop results from picture result suggestions etc. (T/F)
hits <- pbmclapply(search.url[1:10], function(x) get_google_hits(x, raw=F, drop.recursives = T), mc.cores=4)

# filter out all pages with no results
hits <- hits[lengths(hits)>0]

# save hits
save(hits, file=paste0('hits-full-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

# check if they can be accessed or are dead links (is able to handle lists and vectors!)
hits.b <- check_status_code(hits) %>% print(n=100)

# save checks
save(hits.b, file=paste0('hits-checked-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

## use the snowballing algorithm
# NOT RUN: we use a keyword-restriction, only collecting links containing (no-billag|nobillag)
snow.balls <- pbmclapply(hits.b$url[hits.b$boolean==T], function(x){
  cycler(start=x, 
         time.limit=60,
         keywords='no?(-)billag'
  )
}, mc.cores=3)

# save
# save(snow.balls, file=paste0('snowballing-res-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

unique(unlist(snow.balls))

# check them again
snow.balls.b <- check_status_code(snow.balls) %>% print(n=100)

# save(snow.balls.b, file=paste0('snowballing-res-checked-', tolower(gsub('\\s+', '', search.term)), '.RDS'))
load(paste0('snowballing-res-checked-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

snow.balls.b$url[snow.balls.b$boolean==T&!grepl('pdf|PDF|png|PNG|jpeg|jpg|JPEG|JPG', snow.balls.b$url)][310]
## get texts
# add.queries:: add additional queries; default: //p & //title
# preproc.expr:: additional regex expressions for preprocessing
# merged:: if T: return collapsed text (\n-sep); if F :returns a list with character strings (contents for each html-element)
# sort out pdfs/pngs/jpegs!!!!
txt <- pblapply(snow.balls.b$url[snow.balls.b$boolean==T&!grepl('pdf|PDF|png|PNG|jpeg|jpg|JPEG|JPG', snow.balls.b$url)], function(x){
  extract_txt(x,
              merged = FALSE,
              add.queries = c(h2 = '//h2', h3 = '//h3', li='//li'), 
              preproc.expr = '(^(\\s+)?$)|(\\\n(\\s+)?)|(\\s{2,})'
  )
})

# save texts
# save(txt, file=paste0('txt-extrctd-', tolower(gsub('\\s+', '', search.term)), '.RDS'))
load(paste0('txt-extrctd-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

## clean texts
# load list with newspaper name
load('~/share/socsc1/input/liste-ch-zeitungen-wiki.RDS')
# some names a quite problematic, since they are liekly to occur in a normal sentence too...
# let's filter them out
newspapers <- newspapers[!newspapers%in%c('.ch', 
                            'Der Bund', 
                            'Die Hauptstadt',
                            'Die Heimat',
                            'News',
                            'heute',
                            'Saiten',
                            'ZÃ¼rich',
                            'Zollikerberg',
                            'Der Eidgenosse',
                            'Das Volk',
                            'Die Nation',
                            'Die Woche')]

# raw:: if T: only returns annotated texts (tibble); if F: actually computes transformations
# hard.filter:: if grepl returns true on any of these elements, the respective line (== HTML-element) will be removed
# buzzwords:: additional buzzwords (default can be accessed by running `default.buzzwords`)
#         |_ buzzwords are used to indicate possible sclicing points and to compute filter conditions
# min.words:: minimum number of words per single character string (used for filtering, default: 3)
# min.avg.characters:: minimum number of avg.characters per word for each single character string (used for filtering, default: 3)
# max.buzzwords:: max. number of buzzwords allowed per character string (default: 2)
# sclicing:: whether slicing is allowed or not [slicing==deleting last part of text according to user-specified conditions] (T/F)
# slicing.keywords:: character string with additional slicing keywords (default can be accessed by running `default.slicing.keywords`)
#         |_ [!!!] each slicing keywords have to be identical to a buzzword
#         |_ [!!!] HENCE: sclicing.keyword is a subset of buzzwords
#         |_ [!!!] it is recommended to use them very cautiously, since they lead to deletions of whole text parts
# scnd.step.slicing:: last share of the document that will be used to compute the sum buzzwords (default: 3; meaning: last third of the document)
# scnd.step.threshold:: max number of buzzwords allowed in scnd.step.slicing before proceeding to slice the data (default: 4)

# define buzzwords
buzzwords <- c('bild',
               'video',
               'kontakt', 
               'links',
               '(\\&)',
               'registrier',
               'vielen dank',
               'wir wÃ¼nschen ihnen',
               'nzz', 
               'blick',
               'anmeld',
               'passwort',
               'alle rechte',
               'medienwoche',
               '([0-9]{5,})',
               '(\\|)',
               'Â©',
               'app',
               'bitte',
               'kommentar',
               'kompaktansicht',
               'weiterlesen',
               'jetzt spenden',
               'hol dir',
               'â¦',
               'woz',
               '20 minuten',
               'live',
               'resultate',
               'youtube',
               'das kÃ¶nnte sie interessieren',
               'fonction',
               'sauter',
               'navigation',
               'recherche',
               'all rights reserved',
               'external link',
               'click here',
               'newsletter',
               'signing up',
               'swissinfo.ch',
               'copyright',
               'cookies', 
               'watson',
               'radio life',
               'erf medien',
               '\\(c\\)',
               'channel',
               tolower(newspapers)) %>% 
  gsub('\\.', '\\\\.', .)  

# check for correct implementation of punctation
grep('\\.', buzzwords, value=T)

# define additional hardfilter grepl regex
hard.filter <- c('\\((fr|de|it|pt|es|en|ru|ar)\\)', 
                 '([0-9]{2}\\:[0-9]{2}[A-z]+)|([0-9]{2}\\:[0-9]{3}\\s)|([0-9]{2}\\:[0-9]{4}\\.[0-9])', 
                 '([A-Z][a-z]+[A-Z][a-z]+)')

# call cleaner
txt.clean <- pblapply(txt, function(x){
  clean_text(x,
             hard.filter = hard.filter,
             slicing = T,
             min.words = 6,
             max.buzzwords = 3,
             scnd.step.slicing = 3, 
             scnd.step.threshold = 20,
             buzzwords = buzzwords)
})

# save
# save(txt.clean, file=paste0('txt-cleaned-', tolower(gsub('\\s+', '', search.term)), '.RDS'))
load(paste0('txt-cleaned-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

# all texts
txt.clean

# control elements that have been deleted
unlist(sapply(1:length(txt),function(x){
  if(length(txt[[x]])>1){
    # print(x) # untag for error handling
    txt[[x]][!txt[[x]]%in%txt.clean[[x]]$txt]
  }else{NA}}))

# get rid of elments that were single line documents (<- NA), and those that don't contain any information anymore after cleaning
txt.clean <- txt.clean[!is.na(txt.clean)] %>% 
  .[unlist(sapply(., nrow))>1]

# save
# save(txt.clean, file=paste0('txt-cleaned-noNAs-', tolower(gsub('\\s+', '', search.term)), '.RDS'))
load(paste0('txt-cleaned-noNAs-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

# add language variable, merge texts, and convert them to true UTF-8
corp <- tibble(txt.burnt=txt.clean, 
               language=pbsapply(txt.clean, function(x){textcat(paste0(x$txt, collapse = ' '))}), 
               txt.merged=sapply(txt.clean, function(x){paste0(x$txt, collapse = '\n')}),
               txt.merged.utf8=iconv(txt.merged, from = "UTF-8", to = "MAC") %>% iconv(., from = "MAC", to="UTF-8"))
# save
# save(corp, file=paste0('corpus-', tolower(gsub('\\s+', '', search.term)), '.RDS'))
load(paste0('corpus-', tolower(gsub('\\s+', '', search.term)), '.RDS'))

# subset 
corp.de <- filter(corp, language=='german')

# print texts
cat(corp.de$txt.merged.utf8)

# hardcopy of files from dropbox to sharefolder
system('cp -a ~/Dropbox/share/socsc1/output/ ~/share/socsc1/output-directLink/')
```

